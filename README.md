**Prompt Engineering & Evaluation System**
A comprehensive system for evaluating educational prompts covering science and English topics.

**Prompt engineering + evaluation for a generative model**
Objective: Design a set of prompts to get useful outputs from a text-generation model and evaluate robustness/degeneracy.
Deliverable: A catalogue of 10–15 prompt templates, sample outputs, an evaluation rubric (fluency, correctness, bias/ethical check), and a short summary of best/worst prompts.
Tech: Any accessible LLM (Hugging Face endpoint or API), Python scripts to batch-run prompts.
Success: Clear rubric, identification of 2–3 robust prompts and 2 failure modes with mitigation ideas.